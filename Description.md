# model.py
## src_mask & tgt_mask
In the context of the transformer model, the difference between src_mask and tgt_mask lies in their usage within the model.

1. **src_mask (Encoder Mask):**    
- **src_mask** is primarily used in the encoder part of the transformer model.  
- It is employed to mask out padding tokens in the input sequences.  
- The purpose is to ensure that the model doesn’t attend to the padding tokens, which are added to the input sequences to make them uniform in length but don’t carry any meaningful information.  
- Padding token positions are masked with zeros in the **src_mask**.  

2. **tgt_mask (Decoder Mask):**    
- **tgt_mask** is mainly used in the decoder part of the transformer model.  
- It is applied to mask out future tokens in the decoder’s self-attention mechanism and also in the cross-attention mechanism between the decoder and the encoder.    
- The objective is to prevent the model from accessing future information during training and generation.  
- Future token positions in the decoder are masked with zeros in the **tgt_mask**.


The **src_mask** and **tgt_mask** that are returned on dataset.py go through the following operation.

```py
class BilingualDataset(Dataset):
    ...
    def __getitem__(self, idx):
        ...
        return {
            ...
            "encoder_mask": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)
            "decoder_mask": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len)
            ...
        }

    def causal_mask(size):
        mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)
        return mask == 0
```

1. **src_mask (Encoder Mask):**  
- **src_mask** is constructed based on the **encoder_input**, which represents the input sequence to the encoder.  
- It aims to mask out the padding tokens **(self.pad_token)** in the input sequence.  
- The masking operation is performed by creating a binary mask where non-padding positions are marked as 1, and padding positions are marked as 0.  
- The encoder mask is created by checking if each element in the **encoder_input** is equal to the padding token **(self.pad_token)**. This operation results in a boolean mask where True represents non-padding positions and False represents padding positions.
- The boolean mask is then unsqueezed twice to match the expected shape **(1, 1, seq_len)** for compatibility with the subsequent operations in the model, and it’s converted to integers **(int)** to be used in calculations.

2. **tgt_mask (Decoder Mask):**  
- **tgt_mask** is constructed based on the **decoder_input**, which represents the input sequence to the decoder.
- It serves two purposes: masking out the padding tokens and implementing the causal masking to prevent the model from accessing future information during training.  
- Similar to the **src_mask**, padding tokens in the **decoder_input** are masked out by creating a binary mask.  
- Additionally, causal masking is applied to prevent each token from attending to subsequent tokens in the sequence. This is achieved by combining the binary mask for padding tokens with a causal mask generated by the **causal_mask** function. The **causal_mask** function generates a triangular matrix where the upper triangular elements are set to 0, ensuring that each token can only attend to itself and previous tokens in the sequence.

By applying these masks appropriately, the model can effectively process input sequences and generate outputs while considering only the relevant parts of the input and preventing access to future information during training.

The returned src_mask and tgt_mask later process the masked part similar to $-\inf$(-1e9) in the MultiHeadAttentionBlock at model.py as shown in the code below.

```py
class MultiHeadAttentionBlock(nn.Module):
    ...
    def attention(query, key, value, mask, dropout: nn.Dropout):
        ...
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
            ...
```







