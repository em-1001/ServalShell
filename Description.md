# model.py
## src_mask & tgt_mask
In the context of the transformer model, the difference between src_mask and tgt_mask lies in their usage within the model.

1. **src_mask (Encoder Mask):**    
- **src_mask** is primarily used in the encoder part of the transformer model.  
- It is employed to mask out padding tokens in the input sequences.  
- The purpose is to ensure that the model doesn’t attend to the padding tokens, which are added to the input sequences to make them uniform in length but don’t carry any meaningful information.  
- Padding token positions are masked with zeros in the **src_mask**.  

2. **tgt_mask (Decoder Mask):**    
- **tgt_mask** is mainly used in the decoder part of the transformer model.  
- It is applied to mask out future tokens in the decoder’s self-attention mechanism and also in the cross-attention mechanism between the decoder and the encoder.    
- The objective is to prevent the model from accessing future information during training and generation.  
- Future token positions in the decoder are masked with zeros in the **tgt_mask**.


The **src_mask** and **tgt_mask** that are returned on dataset.py go through the following operation.

```py
class BilingualDataset(Dataset):
    ...
    def __getitem__(self, idx):
        ...
        return {
            ...
            "encoder_mask": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)
            "decoder_mask": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len)
            ...
        }

    def causal_mask(size):
        mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)
        return mask == 0
```

1. **src_mask (Encoder Mask):**  
- **src_mask** is constructed based on the **encoder_input**, which represents the input sequence to the encoder.  
- It aims to mask out the padding tokens **(self.pad_token)** in the input sequence.  
- The masking operation is performed by creating a binary mask where non-padding positions are marked as 1, and padding positions are marked as 0.  
- The encoder mask is created by checking if each element in the **encoder_input** is equal to the padding token **(self.pad_token)**. This operation results in a boolean mask where True represents non-padding positions and False represents padding positions.
- The boolean mask is then unsqueezed twice to match the expected shape **(1, 1, seq_len)** for compatibility with the subsequent operations in the model, and it’s converted to integers **(int)** to be used in calculations.

2. **tgt_mask (Decoder Mask):**  
- **tgt_mask** is constructed based on the **decoder_input**, which represents the input sequence to the decoder.
- It serves two purposes: masking out the padding tokens and implementing the causal masking to prevent the model from accessing future information during training.  
- Similar to the **src_mask**, padding tokens in the **decoder_input** are masked out by creating a binary mask.  
- Additionally, causal masking is applied to prevent each token from attending to subsequent tokens in the sequence. This is achieved by combining the binary mask for padding tokens with a causal mask generated by the **causal_mask** function. The **causal_mask** function generates a triangular matrix where the upper triangular elements are set to 0, ensuring that each token can only attend to itself and previous tokens in the sequence.

By applying these masks appropriately, the model can effectively process input sequences and generate outputs while considering only the relevant parts of the input and preventing access to future information during training.

The returned src_mask and tgt_mask later process the masked part similar to $-\infty$(-1e9) in the MultiHeadAttentionBlock at model.py as shown in the code below.

```py
class MultiHeadAttentionBlock(nn.Module):
    ...
    def attention(query, key, value, mask, dropout: nn.Dropout):
        ...
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
            ...
```

# train.py
## ‎get_or_build_tokenizer‎()
1. **get_all_sentences(ds, lang)**:  
This function extracts all sentences from the dataset **ds**. It retrieves the translated sentences corresponding to the given language **lang** and returns them. This function serves as a generator, yielding one sentence at a time.  
2. **get_or_build_tokenizer(config, ds, lang)**:  
This function retrieves or builds a tokenizer for the given language. It takes three arguments:  
- config: A dictionary containing settings related to the path of the tokenizer file.  
- ds: The dataset object.  
- lang: The language for which the tokenizer will be retrieved or built.  
This function first checks if the tokenizer file for the specified language exists. If it does not exist, it learns and saves the tokenizer for that language by extracting all sentences from the dataset. Otherwise, it loads the saved tokenizer file.  
3. **tokenizer_path**:  
This variable represents the path of the tokenizer file. The path is determined based on the values obtained from the configured **config**.  
4. **Tokenizer**:  
This class represents a tokenizer provided by the Hugging Face’s Tokenizers library. It tokenizes text and encodes tokens into numerical values.  
5. **WordLevel**:  
This class represents a word-level tokenizer. Each word is treated as an individual token.  
6. **Whitespace**:  
This class represents a pre-tokenizer that tokenizes text based on whitespace. Words are separated by whitespace.  
7. **WordLevelTrainer**:  
This class is responsible for training the tokenizer. It sets hyperparameters such as minimum frequency to train the tokenizer.  
8. **[UNK], [PAD], [SOS], [EOS]**:  
These tokens represent unknown, padding, start of sentence, and end of sentence tokens, respectively. These special tokens provide additional information to the model or are used during data preprocessing.  















